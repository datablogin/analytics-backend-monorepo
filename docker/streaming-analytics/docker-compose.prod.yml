version: '3.8'

services:
  # Kafka cluster for production
  kafka-1:
    image: confluentinc/cp-kafka:latest
    hostname: kafka-1
    container_name: kafka-1
    ports:
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: SSL:SSL,SSL_HOST:SSL
      KAFKA_ADVERTISED_LISTENERS: SSL://kafka-1:29092,SSL_HOST://kafka-1:9092
      # TLS/SSL Configuration
      KAFKA_SSL_KEYSTORE_FILENAME: kafka.server.keystore.jks
      KAFKA_SSL_KEYSTORE_CREDENTIALS: kafka_keystore_creds
      KAFKA_SSL_KEY_CREDENTIALS: kafka_ssl_key_creds
      KAFKA_SSL_TRUSTSTORE_FILENAME: kafka.server.truststore.jks
      KAFKA_SSL_TRUSTSTORE_CREDENTIALS: kafka_truststore_creds
      KAFKA_SSL_ENDPOINT_IDENTIFICATION_ALGORITHM: " "
      KAFKA_SSL_CLIENT_AUTH: none
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 3
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 2
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 3
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: 'false'
      KAFKA_LOG_RETENTION_HOURS: 168
      KAFKA_LOG_RETENTION_BYTES: 1073741824
      KAFKA_LOG_SEGMENT_BYTES: 536870912
      KAFKA_NUM_PARTITIONS: 6
      KAFKA_DEFAULT_REPLICATION_FACTOR: 3
    volumes:
      - kafka-1-data:/var/lib/kafka/data
    networks:
      - streaming-network
    depends_on:
      - zookeeper
    healthcheck:
      test: ["CMD-SHELL", "kafka-broker-api-versions --bootstrap-server localhost:9092"]
      interval: 30s
      timeout: 10s
      retries: 3

  kafka-2:
    image: confluentinc/cp-kafka:latest
    hostname: kafka-2
    container_name: kafka-2
    ports:
      - "9093:9093"
    environment:
      KAFKA_BROKER_ID: 2
      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: SSL:SSL,SSL_HOST:SSL
      KAFKA_ADVERTISED_LISTENERS: SSL://kafka-2:29092,SSL_HOST://kafka-2:9093
      # TLS/SSL Configuration
      KAFKA_SSL_KEYSTORE_FILENAME: kafka.server.keystore.jks
      KAFKA_SSL_KEYSTORE_CREDENTIALS: kafka_keystore_creds
      KAFKA_SSL_KEY_CREDENTIALS: kafka_ssl_key_creds
      KAFKA_SSL_TRUSTSTORE_FILENAME: kafka.server.truststore.jks
      KAFKA_SSL_TRUSTSTORE_CREDENTIALS: kafka_truststore_creds
      KAFKA_SSL_ENDPOINT_IDENTIFICATION_ALGORITHM: " "
      KAFKA_SSL_CLIENT_AUTH: none
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 3
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 2
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 3
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: 'false'
      KAFKA_LOG_RETENTION_HOURS: 168
      KAFKA_LOG_RETENTION_BYTES: 1073741824
      KAFKA_LOG_SEGMENT_BYTES: 536870912
      KAFKA_NUM_PARTITIONS: 6
      KAFKA_DEFAULT_REPLICATION_FACTOR: 3
    volumes:
      - kafka-2-data:/var/lib/kafka/data
    networks:
      - streaming-network
    depends_on:
      - zookeeper
    healthcheck:
      test: ["CMD-SHELL", "kafka-broker-api-versions --bootstrap-server localhost:9093"]
      interval: 30s
      timeout: 10s
      retries: 3

  kafka-3:
    image: confluentinc/cp-kafka:latest
    hostname: kafka-3
    container_name: kafka-3
    ports:
      - "9094:9094"
    environment:
      KAFKA_BROKER_ID: 3
      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: SSL:SSL,SSL_HOST:SSL
      KAFKA_ADVERTISED_LISTENERS: SSL://kafka-3:29092,SSL_HOST://kafka-3:9094
      # TLS/SSL Configuration
      KAFKA_SSL_KEYSTORE_FILENAME: kafka.server.keystore.jks
      KAFKA_SSL_KEYSTORE_CREDENTIALS: kafka_keystore_creds
      KAFKA_SSL_KEY_CREDENTIALS: kafka_ssl_key_creds
      KAFKA_SSL_TRUSTSTORE_FILENAME: kafka.server.truststore.jks
      KAFKA_SSL_TRUSTSTORE_CREDENTIALS: kafka_truststore_creds
      KAFKA_SSL_ENDPOINT_IDENTIFICATION_ALGORITHM: " "
      KAFKA_SSL_CLIENT_AUTH: none
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 3
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 2
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 3
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: 'false'
      KAFKA_LOG_RETENTION_HOURS: 168
      KAFKA_LOG_RETENTION_BYTES: 1073741824
      KAFKA_LOG_SEGMENT_BYTES: 536870912
      KAFKA_NUM_PARTITIONS: 6
      KAFKA_DEFAULT_REPLICATION_FACTOR: 3
    volumes:
      - kafka-3-data:/var/lib/kafka/data
    networks:
      - streaming-network
    depends_on:
      - zookeeper
    healthcheck:
      test: ["CMD-SHELL", "kafka-broker-api-versions --bootstrap-server localhost:9094"]
      interval: 30s
      timeout: 10s
      retries: 3

  zookeeper:
    image: confluentinc/cp-zookeeper:latest
    hostname: zookeeper
    container_name: zookeeper
    ports:
      - "2181:2181"
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
      ZOOKEEPER_SYNC_LIMIT: 2
      ZOOKEEPER_INIT_LIMIT: 5
    volumes:
      - zookeeper-data:/var/lib/zookeeper/data
      - zookeeper-logs:/var/lib/zookeeper/log
    networks:
      - streaming-network
    healthcheck:
      test: ["CMD-SHELL", "echo ruok | nc localhost 2181 | grep imok"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Streaming Analytics Services
  websocket-server:
    build:
      context: ../..
      dockerfile: docker/streaming-analytics/Dockerfile
    image: streaming-analytics:latest
    container_name: websocket-server
    ports:
      - "8765:8765"
      - "8080:8080"
    environment:
      SERVICE_TYPE: websocket
      STREAMING_ENVIRONMENT: production
      STREAMING_KAFKA__BOOTSTRAP_SERVERS: '["kafka-1:29092","kafka-2:29092","kafka-3:29092"]'
      STREAMING_KAFKA__SECURITY_PROTOCOL: SSL
      STREAMING_WEBSOCKET__MAX_CONNECTIONS: 5000
      DATABASE_URL: postgresql://${DATABASE_USER:-streaming_user}:${DATABASE_PASSWORD:-CHANGE_ME}@postgres:5432/streaming_analytics
      REDIS_URL: redis://redis:6379
    command: ["python", "-m", "libs.streaming_analytics.websocket_server"]
    depends_on:
      kafka-1:
        condition: service_healthy
      kafka-2:
        condition: service_healthy
      kafka-3:
        condition: service_healthy
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    networks:
      - streaming-network
    volumes:
      - websocket-logs:/home/streaming/logs
    deploy:
      replicas: 3
      resources:
        limits:
          cpus: '2.0'
          memory: 4G
        reservations:
          cpus: '0.5'
          memory: 1G
    healthcheck:
      test: ["CMD", "python", "healthcheck.py"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  stream-processor:
    build:
      context: ../..
      dockerfile: docker/streaming-analytics/Dockerfile
    image: streaming-analytics:latest
    environment:
      SERVICE_TYPE: processor
      STREAMING_ENVIRONMENT: production
      STREAMING_KAFKA__BOOTSTRAP_SERVERS: '["kafka-1:29092","kafka-2:29092","kafka-3:29092"]'
      STREAMING_KAFKA__SECURITY_PROTOCOL: SSL
      STREAMING_STREAM_PROCESSING__PARALLELISM: 4
      DATABASE_URL: postgresql://${DATABASE_USER:-streaming_user}:${DATABASE_PASSWORD:-CHANGE_ME}@postgres:5432/streaming_analytics
      REDIS_URL: redis://redis:6379
    command: ["python", "-m", "libs.streaming_analytics.processor"]
    depends_on:
      kafka-1:
        condition: service_healthy
      kafka-2:
        condition: service_healthy
      kafka-3:
        condition: service_healthy
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    networks:
      - streaming-network
    volumes:
      - processor-logs:/home/streaming/logs
      - processor-state:/home/streaming/rocksdb
    deploy:
      replicas: 6
      resources:
        limits:
          cpus: '4.0'
          memory: 8G
        reservations:
          cpus: '1.0'
          memory: 2G
    healthcheck:
      test: ["CMD", "python", "healthcheck.py"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  ml-inference:
    build:
      context: ../..
      dockerfile: docker/streaming-analytics/Dockerfile
    image: streaming-analytics:latest
    ports:
      - "8081:8081"
    environment:
      SERVICE_TYPE: ml-inference
      STREAMING_ENVIRONMENT: production
      STREAMING_KAFKA__BOOTSTRAP_SERVERS: '["kafka-1:29092","kafka-2:29092","kafka-3:29092"]'
      STREAMING_KAFKA__SECURITY_PROTOCOL: SSL
      STREAMING_REALTIME_ML__MODEL_CACHE_SIZE: 10
      DATABASE_URL: postgresql://${DATABASE_USER:-streaming_user}:${DATABASE_PASSWORD:-CHANGE_ME}@postgres:5432/streaming_analytics
      REDIS_URL: redis://redis:6379
    command: ["python", "-m", "libs.streaming_analytics.realtime_ml"]
    depends_on:
      kafka-1:
        condition: service_healthy
      kafka-2:
        condition: service_healthy
      kafka-3:
        condition: service_healthy
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    networks:
      - streaming-network
    volumes:
      - ml-inference-logs:/home/streaming/logs
      - ml-models-cache:/home/streaming/models
    deploy:
      replicas: 4
      resources:
        limits:
          cpus: '8.0'
          memory: 16G
        reservations:
          cpus: '2.0'
          memory: 4G
    healthcheck:
      test: ["CMD", "python", "healthcheck.py"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s

  # Database
  postgres:
    image: postgres:15
    container_name: postgres
    ports:
      - "5432:5432"
    environment:
      POSTGRES_DB: streaming_analytics
      POSTGRES_USER: ${DATABASE_USER:-streaming_user}
      POSTGRES_PASSWORD: ${DATABASE_PASSWORD:-CHANGE_ME}
      POSTGRES_INITDB_ARGS: "--auth-host=scram-sha-256"
    volumes:
      - postgres-data:/var/lib/postgresql/data
      - ./postgres-init:/docker-entrypoint-initdb.d
    networks:
      - streaming-network
    command: >
      postgres -c max_connections=200
               -c shared_buffers=256MB
               -c effective_cache_size=1GB
               -c maintenance_work_mem=64MB
               -c checkpoint_completion_target=0.9
               -c wal_buffers=16MB
               -c default_statistics_target=100
               -c random_page_cost=1.1
               -c effective_io_concurrency=200
               -c work_mem=4MB
               -c min_wal_size=1GB
               -c max_wal_size=4GB
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U streaming_user -d streaming_analytics"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Redis
  redis:
    image: redis:7-alpine
    container_name: redis
    ports:
      - "6379:6379"
    volumes:
      - redis-data:/data
    networks:
      - streaming-network
    command: >
      redis-server --appendonly yes
                   --maxmemory 2gb
                   --maxmemory-policy allkeys-lru
                   --tcp-keepalive 60
                   --timeout 300
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 30s
      timeout: 10s
      retries: 3

networks:
  streaming-network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.20.0.0/16

volumes:
  # Kafka volumes
  kafka-1-data:
  kafka-2-data:
  kafka-3-data:
  zookeeper-data:
  zookeeper-logs:
  
  # Application volumes
  websocket-logs:
  processor-logs:
  processor-state:
  ml-inference-logs:
  ml-models-cache:
  
  # Database volumes
  postgres-data:
  redis-data: